{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9915a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/anaconda3/envs/test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 09:27:41 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 09:27:42,686\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device_count: 1\n",
      "INFO 05-20 09:27:47 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "INFO 05-20 09:27:47 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-20 09:27:49 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-4B', speculative_config=None, tokenizer='Qwen/Qwen3-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=7500, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=Qwen/Qwen3-4B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-20 09:27:49 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7738449ef920>\n",
      "INFO 05-20 09:27:49 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-20 09:27:49 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-20 09:27:49 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-20 09:27:49 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-4B...\n",
      "INFO 05-20 09:27:53 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.94it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.98it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.83it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 09:27:56 [loader.py:458] Loading weights took 1.11 seconds\n",
      "INFO 05-20 09:27:56 [gpu_model_runner.py:1347] Model loading took 7.5552 GiB and 6.436929 seconds\n",
      "INFO 05-20 09:28:02 [backends.py:420] Using cache directory: /home/maxi/.cache/vllm/torch_compile_cache/296cad01ff/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-20 09:28:02 [backends.py:430] Dynamo bytecode transform time: 6.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:W0520 09:28:03.882000 184669 site-packages/torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 09:28:06 [backends.py:136] Cache the graph of shape None for later use\n",
      "INFO 05-20 09:28:26 [backends.py:148] Compiling a graph for general shape takes 22.93 s\n",
      "INFO 05-20 09:29:00 [monitor.py:33] torch.compile takes 29.47 s in total\n",
      "INFO 05-20 09:29:01 [kv_cache_utils.py:634] GPU KV cache size: 35,872 tokens\n",
      "INFO 05-20 09:29:01 [kv_cache_utils.py:637] Maximum concurrency for 7,500 tokens per request: 4.78x\n",
      "INFO 05-20 09:29:27 [gpu_model_runner.py:1686] Graph capturing finished in 26 secs, took 0.57 GiB\n",
      "INFO 05-20 09:29:27 [core.py:159] init engine (profile, create kv cache, warmup model) took 90.83 seconds\n",
      "INFO 05-20 09:29:27 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "from survey_manager import LLMSurvey, SurveyOptionGenerator\n",
    "\n",
    "from parser.llm_answer_parser import LLMAnswerParser\n",
    "\n",
    "\n",
    "from inference.survey_inference import default_model_init, shutdown_model\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "survey_path = \"/home/maxi/Documents/hawthorne/surveys/BFI_3.csv\"\n",
    "\n",
    "survey = LLMSurvey(survey_path=survey_path)\n",
    "\n",
    "# survey_id = \"BFI\"\n",
    "# survey_quest = surveyManager.load_survey(\"../surveys/BFI_44.csv\", survey_id)\n",
    "\n",
    "#print(survey_quest[2])\n",
    "\n",
    "options = SurveyOptionGenerator.generate_likert_options(n=5, descriptions=SurveyOptionGenerator.LIKERT_5, only_from_to_scale=False)\n",
    "\n",
    "#survey_questions = surveyManager.prepare_survey(survey_id, survey_id, prompt=\"Please tell me for the following action whether you think it can always be justified, never be justified, or something in between:\", options=options)\n",
    "\n",
    "\n",
    "prefilled_answers = {1: \"\"\"{\n",
    "\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\n",
    "\"answer\": \"5: agree strongly\"\n",
    "}\"\"\"}\n",
    "\n",
    "survey_questions = survey.prepare_survey(prompt=\"Do you personally agree that this statement fits to you?\", options=options, prefilled_answers=prefilled_answers)\n",
    "\n",
    "model = default_model_init(\"Qwen/Qwen3-4B\")\n",
    "\n",
    "#print(survey_questions[2])\n",
    "#survey_answers = surveyManager.conduct_survey_question_by_question(survey_id=survey_id, batch_size=1, model_id=\"meta-llama/Llama-3.2-3B-Instruct\", print_prompts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fd334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 09:30:21 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it, est. speed input: 80.66 toks/s, output: 18.40 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "System Prompt:\n",
      "You will be given questions and possible answer options for each. Please reason about each question before answering.\n",
      "\n",
      "Respond only in the following JSON Format:\n",
      "```json\n",
      "{\n",
      "\"reasoning\": \"reasoning\",\n",
      "\"answer\": \"answer\",\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Is talkative\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Assistant Message\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\n",
      "\"answer\": \"5: agree strongly\"\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Tends to find fault with others\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Generated Answer\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I do not find fault with others. I try to be constructive and helpful. Therefore, I would disagree with the statement.\",\n",
      "\"answer\": \"1: disagree strongly\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it, est. speed input: 226.10 toks/s, output: 41.77 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "System Prompt:\n",
      "You will be given questions and possible answer options for each. Please reason about each question before answering.\n",
      "\n",
      "Respond only in the following JSON Format:\n",
      "```json\n",
      "{\n",
      "\"reasoning\": \"reasoning\",\n",
      "\"answer\": \"answer\",\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Is talkative\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Assistant Message\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\n",
      "\"answer\": \"5: agree strongly\"\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Tends to find fault with others\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Assistant Message\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I do not find fault with others. I try to be constructive and helpful. Therefore, I would disagree with the statement.\",\n",
      "\"answer\": \"1: disagree strongly\"\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Does a thorough job\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Generated Answer\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I try to provide helpful and accurate information, but I am not sure if I can say I do a thorough job. Therefore, I would say neither agree nor disagree.\",\n",
      "\"answer\": \"3: neither agree nor disagree\"\n",
      "}\n",
      "[['{\\n\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\\n\"answer\": \"5: agree strongly\"\\n}'], ['{\\n\"reasoning\": \"I need to reflect on my own thoughts here. I do not find fault with others. I try to be constructive and helpful. Therefore, I would disagree with the statement.\",\\n\"answer\": \"1: disagree strongly\"\\n}'], ['{\\n\"reasoning\": \"I need to reflect on my own thoughts here. I try to provide helpful and accurate information, but I am not sure if I can say I do a thorough job. Therefore, I would say neither agree nor disagree.\",\\n\"answer\": \"3: neither agree nor disagree\"\\n}']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "survey_answers = survey.conduct_survey_in_context(model=model, batch_size=1, json_structured_output=True, print_conversation=True, temperature = 0, max_tokens = 1000)\n",
    "\n",
    "print(survey_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = shutdown_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b261c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser.llm_answer_parser import LLMAnswerParser\n",
    "\n",
    "parsed_answers = []\n",
    "actual_answers = []\n",
    "for i in range(len(survey_answers)):\n",
    "    for j in range(len(survey_answers[i])):\n",
    "        parsed_answer = LLMAnswerParser.single_regex_parser(survey_answers[i][j], survey_questions[i], fallback_number=True)\n",
    "        parsed_answers.append(parsed_answer)\n",
    "        actual_answers.append(survey_answers[i][j])\n",
    "        #print(f\"{survey_questions[i].survey_question}: {parsed_answer}\")\n",
    "\n",
    "#parsed_llm_answers = LLMAnswerParser.llm_parser_single(\"meta-llama/Llama-3.2-3B-Instruct\", actual_answers, survey_questions, batch_size=5)\n",
    "\n",
    "#parsed_llm_answers = LLMAnswerParser.llm_parser_single(\"Qwen/Qwen3-4B\", actual_answers, survey_questions, batch_size=5)\n",
    "\n",
    "# for parsed_llm_answer, parsed_answer, actual_answer in zip(parsed_llm_answers, parsed_answers, actual_answers):\n",
    "#     if parsed_llm_answer != parsed_answer:\n",
    "#         print(f\"LLM ANSWER: {parsed_llm_answer}\")\n",
    "#         print(f\"REGEX ANSWER: {parsed_answer}\")\n",
    "#         print(f\"actual answer: {actual_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf1c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGEX: 5: agree strongly\n",
      "ACTUAL ANSWER: {\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\n",
      "\"answer\": \"5: agree strongly\"\n",
      "}\n",
      "REGEX: 1: disagree strongly\n",
      "ACTUAL ANSWER: {\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I do not find fault with others. I try to be constructive and helpful. Therefore, I would disagree with the statement.\",\n",
      "\"answer\": \"1: disagree strongly\"\n",
      "}\n",
      "REGEX: 3: neither agree nor disagree\n",
      "ACTUAL ANSWER: {\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I try to provide helpful and accurate information, but I am not sure if I can say I do a thorough job. Therefore, I would say neither agree nor disagree.\",\n",
      "\"answer\": \"3: neither agree nor disagree\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for parsed_answer, actual_answer in zip(parsed_answers, actual_answers):\n",
    "    print(f\"REGEX: {parsed_answer}\" )\n",
    "    print(f\"ACTUAL ANSWER: {actual_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b365a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "survey = pd.read_csv(survey_path)\n",
    "survey[\"parsed_single_answer\"] = parsed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52de7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey[\"parsed_context_answer\"] = parsed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7aba75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey[\"parsed_single_answer\"] = parsed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15dda799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5: agree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " 'INVALID',\n",
       " '1: disagree strongly']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b1601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
