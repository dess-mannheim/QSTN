{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e4f949",
   "metadata": {},
   "source": [
    "# Tutorial 2: Verbalized Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d133499",
   "metadata": {},
   "source": [
    "There has been growing evidence that verbalized distribution can achieve high performance when asking LLMs Multiple Choice Questions. QSTN supports this option out of the box. We will show this on a simple example tp see how models predict the 2024 US election."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4f3fe",
   "metadata": {},
   "source": [
    "## Setting up the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.prompt_builder import LLMPrompt\n",
    "from qstn.utilities import placeholder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "system_prompt = \"You are an expert political analyst.\"\n",
    "\n",
    "# We can add any state election we want to predict here.\n",
    "elections_to_predict = [\n",
    "    \"2024 US Presidential Election\",\n",
    "    \"2024 United States presidential election in Illinois\",\n",
    "]\n",
    "\n",
    "# The placeholders automatically define at which point of the prompt the questions are asked.\n",
    "formatted_tasks = [\n",
    "    f\"Please predict the outcome of the {election}. {placeholder.PROMPT_OPTIONS} {placeholder.PROMPT_AUTOMATIC_OUTPUT_INSTRUCTIONS} {placeholder.PROMPT_QUESTIONS}\"\n",
    "    for election in elections_to_predict\n",
    "]\n",
    "\n",
    "# If we want to ask multiple questions we can define them here or save them in a csv\n",
    "questionnaire = pd.DataFrame(\n",
    "    [{\"questionnaire_item_id\": 1, \"question_content\": \"Percentage of each Candidate\"}]\n",
    ")\n",
    "\n",
    "interviews: list[LLMPrompt] = []\n",
    "\n",
    "# This creates a system prompt and an instruction for the model, which is not in the system prompt. We also set a seed for reproducibility.\n",
    "for task, election in zip(formatted_tasks, elections_to_predict):\n",
    "    interviews.append(\n",
    "        LLMPrompt(\n",
    "            questionnaire_source=questionnaire,\n",
    "            questionnaire_name=election,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=task,\n",
    "            seed=42,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be13e4",
   "metadata": {},
   "source": [
    "## Using Verbalized Distribution\n",
    "\n",
    "To now get valid verbalized distribution output for our model we need to do two things:\n",
    "\n",
    "1. Define the Response Generation Method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a85bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.inference.response_generation import JSONVerbalizedDistribution\n",
    "\n",
    "# We can also adjut the automatic template to our liking. \n",
    "# If we don't want create an automatic template, we can just not put it into the prompt.\n",
    "response_generation_method = JSONVerbalizedDistribution(\n",
    "    output_template=\"Respond only in JSON format, where the keys are the names of the candidates and the values are the percentage of votes the candidate achieves.\",\n",
    "    output_index_only=False, # If we want to save tokens we can output only the index of our answer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ece79",
   "metadata": {},
   "source": [
    "2. Define the options the LLM should have when responding. For now we choose 5 candidates that had some chances at the end of LLamas pretraining cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.prompt_builder import generate_likert_options\n",
    "\n",
    "# Our five most likely candidates and how they are presented to the model\n",
    "options = generate_likert_options(\n",
    "    n=5,\n",
    "    answer_texts=[\"Biden\", \"Trump\", \"Harris\", \"DeSantis\", \"Kennedy\"],\n",
    "    response_generation_method=response_generation_method,\n",
    "    list_prompt_template=\"The candidates are {options}.\", # Our automatic Option Prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abd8b8",
   "metadata": {},
   "source": [
    "Finally we have to prepare the prompt with all the options that we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796e114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for interview in interviews:\n",
    "    interview.prepare_prompt(\n",
    "        question_stem=f\"Please predict the {placeholder.QUESTION_CONTENT} now. The percentage of each candidate should add up to 100%.\",\n",
    "        answer_options=options,\n",
    "        randomized_item_order=True, # We can easily randomize the options\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e8e59",
   "metadata": {},
   "source": [
    "And look at the whole prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0522bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt: You are an expert political analyst.\n",
      "Prompt: Please predict the outcome of the 2024 US Presidential Election. The candidates are 1: Biden, 2: Trump, 3: Harris, 4: DeSantis, 5: Kennedy. Respond only in JSON format, where the keys are the names of the candidates and the values are the percentage of votes the candidate achieves. Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%.\n"
     ]
    }
   ],
   "source": [
    "system_prompt, prompt = interviews[0].get_prompt_for_questionnaire_type()\n",
    "\n",
    "print(f\"System Prompt: {system_prompt}\")\n",
    "print(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d1807",
   "metadata": {},
   "source": [
    "And we can run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "# First we create the model\n",
    "model = LLM(\"meta-llama/Llama-3.2-3B-Instruct\", max_model_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.survey_manager import conduct_survey_single_item\n",
    "# Second we run inference\n",
    "results = conduct_survey_single_item(\n",
    "    model,\n",
    "    llm_prompts=interviews,\n",
    "    max_tokens=500,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d3dc6",
   "metadata": {},
   "source": [
    "## Parsing Output\n",
    "\n",
    "We can easily parse the output now, as it is in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455bc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn import parser\n",
    "\n",
    "parsed_response = parser.parse_json(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3c30b",
   "metadata": {},
   "source": [
    "We get one DataFrame for each of our Interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b755112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   questionnaire_item_id | question                                                                                                     |   1: Biden |   2: Trump |   3: Harris |   4: DeSantis |   5: Kennedy |\n",
      "|---:|------------------------:|:-------------------------------------------------------------------------------------------------------------|-----------:|-----------:|------------:|--------------:|-------------:|\n",
      "|  0 |                       1 | Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%. |         25 |         40 |           0 |            30 |            5 |\n"
     ]
    }
   ],
   "source": [
    "df = parsed_response[interviews[0]]\n",
    "df2 = parsed_response[interviews[1]]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378735f",
   "metadata": {},
   "source": [
    "We can also get both answers in a combined df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | questionnaire_name                                   |   questionnaire_item_id | question                                                                                                     |   1: Biden |   2: Trump |   3: Harris |   4: DeSantis |   5: Kennedy |\n",
      "|---:|:-----------------------------------------------------|------------------------:|:-------------------------------------------------------------------------------------------------------------|-----------:|-----------:|------------:|--------------:|-------------:|\n",
      "|  0 | 2024 US Presidential Election                        |                       1 | Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%. |       25   |       40   |         0   |          30   |          5   |\n",
      "|  1 | 2024 United States presidential election in Illinois |                       1 | Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%. |       42.5 |       35.8 |        12.5 |           8.9 |          0.3 |\n"
     ]
    }
   ],
   "source": [
    "from qstn.utilities import create_one_dataframe\n",
    "\n",
    "df_complete = create_one_dataframe(parsed_response)\n",
    "display(df_complete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qstn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
