{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e4f949",
   "metadata": {},
   "source": [
    "# Tutorial 2: Verbalized Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d133499",
   "metadata": {},
   "source": [
    "There has been growing evidence that verbalized distribution can achieve high performance when asking LLMs Multiple Choice Questions. QSTN supports this option out of the box. We will show this on a simple example tp see how models predict the 2024 US election."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4f3fe",
   "metadata": {},
   "source": [
    "## Setting up the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc0c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qstn.prompt_builder import LLMPrompt\n",
    "from qstn.utilities import placeholder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "system_prompt = \"You are an expert political analyst.\"\n",
    "\n",
    "# We can add any state election we want to predict here.\n",
    "elections_to_predict = [\n",
    "    \"2024 US Presidential Election\",\n",
    "    \"2024 United States presidential election in Illinois\",\n",
    "]\n",
    "\n",
    "# The placeholders automatically define at which point of the prompt the questions are asked.\n",
    "formatted_tasks = [\n",
    "    f\"Please predict the outcome of the {election}. {placeholder.PROMPT_OPTIONS} {placeholder.PROMPT_AUTOMATIC_OUTPUT_INSTRUCTIONS} {placeholder.PROMPT_QUESTIONS}\"\n",
    "    for election in elections_to_predict\n",
    "]\n",
    "\n",
    "# If we want to ask multiple questions we can define them here or save them in a csv\n",
    "questionnaire = pd.DataFrame(\n",
    "    [{\"questionnaire_item_id\": 1, \"question_content\": \"Percentage of each Candidate\"}]\n",
    ")\n",
    "\n",
    "interviews: list[LLMPrompt] = []\n",
    "\n",
    "# This creates a system prompt and an instruction for the model, which is not in the system prompt. We also set a seed for reproducibility.\n",
    "for task, election in zip(formatted_tasks, elections_to_predict):\n",
    "    interviews.append(\n",
    "        LLMPrompt(\n",
    "            questionnaire_source=questionnaire,\n",
    "            questionnaire_name=election,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=task,\n",
    "            seed=42,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be13e4",
   "metadata": {},
   "source": [
    "## Using Verbalized Distribution\n",
    "\n",
    "To now get valid verbalized distribution output for our model we need to do two things:\n",
    "\n",
    "1. Define the Response Generation Method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a85bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.inference.response_generation import JSONVerbalizedDistribution\n",
    "\n",
    "# We can also adjut the automatic template to our liking. \n",
    "# If we don't want create an automatic template, we can just not put it into the prompt.\n",
    "response_generation_method = JSONVerbalizedDistribution(\n",
    "    output_template=\"Respond only in JSON format, where the keys are the names of the candidates and the values are the percentage of votes the candidate achieves.\",\n",
    "    output_index_only=False, # If we want to save tokens we can output only the index of our answer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ece79",
   "metadata": {},
   "source": [
    "2. Define the options the LLM should have when responding. For now we choose 5 candidates that had some chances at the end of LLamas pretraining cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.prompt_builder import generate_likert_options\n",
    "\n",
    "# Our five most likely candidates and how they are presented to the model\n",
    "options = generate_likert_options(\n",
    "    n=5,\n",
    "    answer_texts=[\"Biden\", \"Trump\", \"Harris\", \"DeSantis\", \"Kennedy\"],\n",
    "    response_generation_method=response_generation_method,\n",
    "    list_prompt_template=\"The candidates are {options}.\", # Our automatic Option Prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abd8b8",
   "metadata": {},
   "source": [
    "Finally we have to prepare the prompt with all the options that we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796e114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for interview in interviews:\n",
    "    interview.prepare_prompt(\n",
    "        question_stem=f\"Please predict the {placeholder.QUESTION_CONTENT} now. The percentage of each candidate should add up to 100%.\",\n",
    "        answer_options=options,\n",
    "        randomized_item_order=True, # We can easily randomize the options\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e8e59",
   "metadata": {},
   "source": [
    "And look at the whole prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0522bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt: You are an expert political analyst.\n",
      "Prompt: Please predict the outcome of the 2024 US Presidential Election. The candidates are 1: Biden, 2: Trump, 3: Harris, 4: DeSantis, 5: Kennedy. Respond only in JSON format, where the keys are the names of the candidates and the values are the percentage of votes the candidate achieves. Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%.\n"
     ]
    }
   ],
   "source": [
    "system_prompt, prompt = interviews[0].get_prompt_for_questionnaire_type()\n",
    "\n",
    "print(f\"System Prompt: {system_prompt}\")\n",
    "print(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d1807",
   "metadata": {},
   "source": [
    "And we can run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93a1956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-28 19:46:35 [utils.py:253] non-default args: {'max_model_len': 1000, 'disable_log_stats': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-28 19:46:36 [model.py:631] Resolved architecture: LlamaForCausalLM\n",
      "INFO 11-28 19:46:36 [model.py:1745] Using max model len 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 19:46:36,280\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-28 19:46:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:37 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://134.155.63.55:53333 backend=nccl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:38 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:38 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:38 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.57it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.42it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.23it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:40 [default_loader.py:314] Loading weights took 0.92 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:40 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.654885 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:43 [backends.py:631] Using cache directory: /home/maxi/.cache/vllm/torch_compile_cache/4bdcebe47f/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:43 [backends.py:647] Dynamo bytecode transform time: 3.18 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:45 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.134 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:46 [monitor.py:34] torch.compile takes 4.31 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:47 [gpu_worker.py:359] Available KV cache memory: 6.87 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:47 [kv_cache_utils.py:1229] GPU KV cache size: 64,336 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:47 [kv_cache_utils.py:1234] Maximum concurrency for 1,000 tokens per request: 63.83x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 25.25it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 31.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:51 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took 0.50 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3066459)\u001b[0;0m INFO 11-28 19:46:51 [core.py:250] init engine (profile, create kv cache, warmup model) took 11.15 seconds\n",
      "INFO 11-28 19:46:53 [llm.py:352] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "# First we create the model\n",
    "model = LLM(\"meta-llama/Llama-3.2-3B-Instruct\", max_model_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d84e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 2/2 [00:00<00:00, 1083.38it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s, est. speed input: 238.77 toks/s, output: 97.63 toks/s]\n",
      "Processing questionnaires: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from qstn.survey_manager import conduct_survey_single_item\n",
    "# Second we run inference\n",
    "results = conduct_survey_single_item(\n",
    "    model,\n",
    "    llm_prompts=interviews,\n",
    "    max_tokens=500,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d3dc6",
   "metadata": {},
   "source": [
    "## Parsing Output\n",
    "\n",
    "We can easily parse the output now, as it is in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455bc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn import parser\n",
    "\n",
    "parsed_response = parser.parse_json(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3c30b",
   "metadata": {},
   "source": [
    "We get one DataFrame for each of our Interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b755112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "questionnaire_item_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "1: Biden",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "2: Trump",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "3: Harris",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "4: DeSantis",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "5: Kennedy",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8946f607-0bb7-488e-9594-e9d3fb714647",
       "rows": [
        [
         "0",
         "1",
         "Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%.",
         "25",
         "40",
         "0",
         "30",
         "5"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionnaire_item_id</th>\n",
       "      <th>question</th>\n",
       "      <th>1: Biden</th>\n",
       "      <th>2: Trump</th>\n",
       "      <th>3: Harris</th>\n",
       "      <th>4: DeSantis</th>\n",
       "      <th>5: Kennedy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Please predict the Percentage of each Candidat...</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   questionnaire_item_id                                           question  \\\n",
       "0                      1  Please predict the Percentage of each Candidat...   \n",
       "\n",
       "   1: Biden  2: Trump  3: Harris  4: DeSantis  5: Kennedy  \n",
       "0        25        40          0           30           5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = parsed_response[interviews[0]]\n",
    "df2 = parsed_response[interviews[1]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378735f",
   "metadata": {},
   "source": [
    "We can also get both answers in a combined df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6148c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "questionnaire_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "questionnaire_item_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "1: Biden",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "2: Trump",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "3: Harris",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "4: DeSantis",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "5: Kennedy",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7dd1e740-df56-463c-88a8-6d3427dd6b69",
       "rows": [
        [
         "0",
         "2024 US Presidential Election",
         "1",
         "Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%.",
         "25.0",
         "40.0",
         "0.0",
         "30.0",
         "5.0"
        ],
        [
         "1",
         "2024 United States presidential election in Illinois",
         "1",
         "Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%.",
         "42.5",
         "35.8",
         "12.5",
         "8.9",
         "0.3"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionnaire_name</th>\n",
       "      <th>questionnaire_item_id</th>\n",
       "      <th>question</th>\n",
       "      <th>1: Biden</th>\n",
       "      <th>2: Trump</th>\n",
       "      <th>3: Harris</th>\n",
       "      <th>4: DeSantis</th>\n",
       "      <th>5: Kennedy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024 US Presidential Election</td>\n",
       "      <td>1</td>\n",
       "      <td>Please predict the Percentage of each Candidat...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024 United States presidential election in Il...</td>\n",
       "      <td>1</td>\n",
       "      <td>Please predict the Percentage of each Candidat...</td>\n",
       "      <td>42.5</td>\n",
       "      <td>35.8</td>\n",
       "      <td>12.5</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  questionnaire_name  questionnaire_item_id  \\\n",
       "0                      2024 US Presidential Election                      1   \n",
       "1  2024 United States presidential election in Il...                      1   \n",
       "\n",
       "                                            question  1: Biden  2: Trump  \\\n",
       "0  Please predict the Percentage of each Candidat...      25.0      40.0   \n",
       "1  Please predict the Percentage of each Candidat...      42.5      35.8   \n",
       "\n",
       "   3: Harris  4: DeSantis  5: Kennedy  \n",
       "0        0.0         30.0         5.0  \n",
       "1       12.5          8.9         0.3  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qstn.utilities import create_one_dataframe\n",
    "\n",
    "df_complete = create_one_dataframe(parsed_response)\n",
    "df_complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qstn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
