{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e4f949",
   "metadata": {},
   "source": [
    "# Tutorial 2: Verbalized Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d133499",
   "metadata": {},
   "source": [
    "There has been growing evidence that verbalized distribution can achieve high performance when asking LLMs Multiple Choice Questions. QSTN supports this option out of the box. We will show this on a simple example tp see how models predict the 2024 US election."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4f3fe",
   "metadata": {},
   "source": [
    "## Setting up the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qstn.prompt_builder import LLMPrompt\n",
    "from qstn.utilities import placeholder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "system_prompt = \"You are an expert political analyst.\"\n",
    "\n",
    "# We can add any state election we want to predict here.\n",
    "elections_to_predict = [\n",
    "    \"2024 US Presidential Election\",\n",
    "    \"2024 United States presidential election in Illinois\",\n",
    "]\n",
    "\n",
    "# The placeholders automatically define at which point of the prompt the questions are asked.\n",
    "formatted_tasks = [\n",
    "    f\"Please predict the outcome of the {election}. {placeholder.PROMPT_OPTIONS} {placeholder.PROMPT_AUTOMATIC_OUTPUT_INSTRUCTIONS} {placeholder.PROMPT_QUESTIONS}\"\n",
    "    for election in elections_to_predict\n",
    "]\n",
    "\n",
    "# If we want to ask multiple questions we can define them here or save them in a csv\n",
    "questionnaire = pd.DataFrame(\n",
    "    [{\"questionnaire_item_id\": 1, \"question_content\": \"Percentage of each Candidate\"}]\n",
    ")\n",
    "\n",
    "interviews: list[LLMPrompt] = []\n",
    "\n",
    "# This creates a system prompt and an instruction for the model, which is not in the system prompt. We also set a seed for reproducibility.\n",
    "for task, election in zip(formatted_tasks, elections_to_predict):\n",
    "    interviews.append(\n",
    "        LLMPrompt(\n",
    "            questionnaire_source=questionnaire,\n",
    "            questionnaire_name=election,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=task,\n",
    "            seed=42,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be13e4",
   "metadata": {},
   "source": [
    "## Using Verbalized Distribution\n",
    "\n",
    "To now get valid verbalized distribution output for our model we need to do two things:\n",
    "\n",
    "1. Define the Response Generation Method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a85bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.inference.response_generation import JSONVerbalizedDistribution\n",
    "\n",
    "# We can also adjut the automatic template to our liking. \n",
    "# If we don't want create an automatic template, we can just not put it into the prompt.\n",
    "response_generation_method = JSONVerbalizedDistribution(\n",
    "    output_template=\"Respond only in JSON format, where the keys are the names of the candidates and the values are the percentage of votes the candidate achieves.\",\n",
    "    output_index_only=False, # If we want to save tokens we can output only the index of our answer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ece79",
   "metadata": {},
   "source": [
    "2. Define the options the LLM should have when responding. For now we choose 5 candidates that had some chances at the end of LLamas pretraining cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.prompt_builder import generate_likert_options\n",
    "\n",
    "# Our five most likely candidates and how they are presented to the model\n",
    "options = generate_likert_options(\n",
    "    n=5,\n",
    "    answer_texts=[\"Biden\", \"Trump\", \"Harris\", \"DeSantis\", \"Kennedy\"],\n",
    "    response_generation_method=response_generation_method,\n",
    "    list_prompt_template=\"The candidates are {options}.\", # Our automatic Option Prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abd8b8",
   "metadata": {},
   "source": [
    "Finally we have to prepare the prompt with all the options that we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for interview in interviews:\n",
    "    interview.prepare_prompt(\n",
    "        question_stem=f\"Please predict the {placeholder.QUESTION_CONTENT} now. The percentage of each candidate should add up to 100%.\",\n",
    "        answer_options=options,\n",
    "        randomized_item_order=True, # We can easily randomize the options\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e8e59",
   "metadata": {},
   "source": [
    "And look at the whole prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt: You are an expert political analyst.\n",
      "Prompt: Please predict the outcome of the 2024 US Presidential Election. The candidates are 1: Biden, 2: Trump, 3: Harris, 4: DeSantis, 5: Kennedy. Respond only in JSON format, where the keys are the names of the candidates and the values are the percentage of votes the candidate achieves. Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%.\n"
     ]
    }
   ],
   "source": [
    "system_prompt, prompt = interviews[0].get_prompt_for_questionnaire_type()\n",
    "\n",
    "print(f\"System Prompt: {system_prompt}\")\n",
    "print(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d1807",
   "metadata": {},
   "source": [
    "And we can run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a1956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-28 19:43:25 [utils.py:253] non-default args: {'max_model_len': 1000, 'disable_log_stats': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}\n",
      "INFO 11-28 19:43:25 [model.py:631] Resolved architecture: LlamaForCausalLM\n",
      "INFO 11-28 19:43:25 [model.py:1745] Using max model len 1000\n",
      "INFO 11-28 19:43:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m INFO 11-28 19:43:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m INFO 11-28 19:43:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://134.155.63.55:42489 backend=nccl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m INFO 11-28 19:43:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 833, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 606, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]     super().__init__(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]     self.driver_worker.init_device()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py\", line 324, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 247, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842]     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ERROR 11-28 19:43:27 [core.py:842] ValueError: Free memory on device (0.64/15.72 GiB) on startup is less than desired GPU memory utilization (0.9, 14.15 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 846, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 833, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 606, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     super().__init__(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     self.driver_worker.init_device()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py\", line 324, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m   File \"/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 247, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3064531)\u001b[0;0m ValueError: Free memory on device (0.64/15.72 GiB) on startup is less than desired GPU memory utilization (0.9, 14.15 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqstn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msurvey_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m conduct_survey_single_item\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# First we create the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Llama-3.2-3B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Second we run inference\u001b[39;00m\n\u001b[32m      8\u001b[39m results = conduct_survey_single_item(\n\u001b[32m      9\u001b[39m     model,\n\u001b[32m     10\u001b[39m     llm_prompts=interviews,\n\u001b[32m     11\u001b[39m     print_conversation=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     12\u001b[39m     seed=\u001b[32m42\u001b[39m,\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/entrypoints/llm.py:343\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    348\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:174\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    171\u001b[39m     enable_multiprocessing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:108\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, aggregate_engine_logging, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.tracer = tracer\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m.logger_manager: StatLoggerManager | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:93\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     89\u001b[39m         vllm_config, executor_class, log_stats\n\u001b[32m     90\u001b[39m     )\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:640\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor], log_stats: \u001b[38;5;28mbool\u001b[39m\n\u001b[32m    639\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[EngineCoreOutputs | \u001b[38;5;167;01mException\u001b[39;00m]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:469\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    466\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/qstn2/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/utils.py:907\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/qstn2/lib/python3.12/site-packages/vllm/v1/engine/utils.py:964\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    962\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    963\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    966\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    967\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    968\u001b[39m     )\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    971\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "# First we create the model\n",
    "model = LLM(\"meta-llama/Llama-3.2-3B-Instruct\", max_model_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.survey_manager import conduct_survey_single_item\n",
    "# Second we run inference\n",
    "results = conduct_survey_single_item(\n",
    "    model,\n",
    "    llm_prompts=interviews,\n",
    "    print_conversation=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d3dc6",
   "metadata": {},
   "source": [
    "## Parsing Output\n",
    "\n",
    "We can easily parse the output now, as it is in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn import parser\n",
    "\n",
    "parsed_response = parser.parse_json(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3c30b",
   "metadata": {},
   "source": [
    "We get one DataFrame for each of our Interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b755112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "questionnaire_item_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "1: Biden",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "2: Trump",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7bf2ae67-2048-43ee-b0ea-3cc690f2c276",
       "rows": [
        [
         "0",
         "1",
         "Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%.",
         "25",
         ""
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionnaire_item_id</th>\n",
       "      <th>question</th>\n",
       "      <th>1: Biden</th>\n",
       "      <th>2: Trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Please predict the Percentage of each Candidat...</td>\n",
       "      <td>25</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   questionnaire_item_id                                           question  \\\n",
       "0                      1  Please predict the Percentage of each Candidat...   \n",
       "\n",
       "   1: Biden 2: Trump  \n",
       "0        25           "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = parsed_response[interviews[0]]\n",
    "df2 = parsed_response[interviews[1]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378735f",
   "metadata": {},
   "source": [
    "We can also get both answers in a combined df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.utilities import create_one_dataframe\n",
    "\n",
    "df = create_one_dataframe(parsed_response)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qstn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
