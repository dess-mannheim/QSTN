{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e828eeca",
   "metadata": {},
   "source": [
    "# German General Personas\n",
    "\n",
    "**This resource allows you to ask questions to a representative sample of 5,246 individual personas representing the German population.**\n",
    "\n",
    "This persona collection consists of **5,246 individual personas representing the German population**. The data source is the [German General Survey (ALLBUScompact)](https://www.gesis.org/en/allbus). Their two-step randomized sampling ensures that ALLBUS as well as the **German General Personas** reflect a representative picture of the German population, regarding its sociodemographic attributes, norms and values.\n",
    "\n",
    "The persona collection was first published in the work [German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies](https://www.arxiv.org/abs/2511.21722).\n",
    "\n",
    "\n",
    "## Why German General Personas? \n",
    "\n",
    "GGP offers several significant advantages:\n",
    "\n",
    "- **Contextual Information**: Personas enrich language models with relevant contextual information, enabling them to anchor predictions for specific tasks or target variables in empirically observed associations and connections within the German population.\n",
    "- **Representative Alignment**: The ALLBUS is a probability-based survey, and the personas derived from it are designed to represent the German population accurately. While there's growing concern about biased representations in LLMs' survey responses, GGP can potentially help align LLMs more effectively with the demographics and attitudes of the German population.\n",
    "- **Novel Resource**: GGP stands as a novel textual resource for researchers and practitioners in Natural Language Processing (NLP) and Computational Social Science (CSS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac98f97",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "First, all relevant Python packages must be imported, such as pandas and **QSTN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc84810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import re\n",
    "\n",
    "# Either local inference with vllm or remote with AsyncOpenAI\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227b64d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homes/jens/.conda/envs/jens_qstn/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/homes/jens/.conda/envs/jens_qstn/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# qstn Imports\n",
    "from qstn.survey_manager import conduct_survey_single_item\n",
    "from qstn.parser import parse_json, raw_responses\n",
    "from qstn.utilities import create_one_dataframe\n",
    "\n",
    "from qstn.prompt_builder import LLMPrompt, generate_likert_options\n",
    "from qstn.utilities import placeholder\n",
    "from qstn.utilities import AnswerOptions\n",
    "\n",
    "from qstn.inference import response_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38545590",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d1097",
   "metadata": {},
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa5b8c",
   "metadata": {},
   "source": [
    "We will load the personas from the [GGP repository](https://github.com/germanpersonas/German-General-Personas).\n",
    "\n",
    "You can define how many personas you want to load and conduct interviews with. In total, you can choose up to 5,246 personas. If you choose less than the available amount of personas, we randomly select personas to ensure a representative sample of the GGP collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a42398",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONAS_TO_LOAD = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c73f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 rows.\n",
      "   index                                            persona\n",
      "0   3811  Du bist eine Person, 26 Jahre alt, männlich un...\n",
      "1   2976  Du bist eine Person, 42 Jahre alt, weiblich, m...\n",
      "2   1387  Du bist eine Person, 38 Jahre alt, weiblich un...\n",
      "3   3773  Du bist eine Person, 54 Jahre alt, wohnhaft in...\n",
      "4   2803  Du bist eine Person, 20 Jahre alt, weiblich, a...\n"
     ]
    }
   ],
   "source": [
    "zip_url = \"https://github.com/germanpersonas/German-General-Personas/raw/main/GGP_all_topk_fulltext.zip\"\n",
    "\n",
    "response = requests.get(zip_url)\n",
    "response.raise_for_status()  # Check for errors\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    with z.open(\"pc_fulltext_sociodemographics_only.jsonl\") as f:\n",
    "        if PERSONAS_TO_LOAD:\n",
    "            df_personas = pd.read_json(f, lines=True)\n",
    "            df_personas = df_personas.sample(n=PERSONAS_TO_LOAD).reset_index(drop=False)\n",
    "        else:\n",
    "            df_personas = pd.read_json(f, lines=True)\n",
    "\n",
    "df_personas = df_personas.rename(columns={0: \"persona\"})\n",
    "print(f\"Loaded {len(df_personas)} rows.\")\n",
    "print(df_personas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c85a1-3962-4950-b354-ae6c4509597d",
   "metadata": {},
   "source": [
    "In addition to the personas, we load example questions and answer options to conduct the survey. \n",
    "\n",
    "However, with **QSTN you can freely create or choose the questions you are asking a representative sample of the German population**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4169d86a-4c1e-4eaf-ac6c-93a698860146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded as dictionary:\n",
      "{'lp04': {'task_type': 'question', 'statement': \"Sind Sie bei der folgenden Aussage derselben oder anderer Meinung: 'So wie die Zukunft aussieht, kann man es kaum noch verantworten, Kinder auf die Welt zu bringen.'?\", 'answers': ['1: BIN DERS.MEINUNG', '2: BIN ANDERER MEINUNG']}, 'pe05': {'task_type': 'question', 'statement': \"Inwiefern stimmen Sie der folgenden Meinung zu: 'Die Politiker bemühen sich im Allgemeinen darum, die Interessen der Bevölkerung zu vertreten.'? (Antwortmöglichkeiten: voll und ganz zustimmen, eher zustimmen, eher nicht zustimmen, überhaupt nicht zustimmen)\", 'answers': ['1: STIMME VOLL ZU', '2: STIMME EHER ZU', '3: STIMME EHER NICHT ZU', '4: STIMME GAR NICHT ZU']}, 'mp18': {'task_type': 'question', 'statement': 'Ergeben sich Ihrer Meinung nach wegen der Flüchtlinge in Bezug auf das Zusammenleben in der Gesellschaft mehr Chancen, mehr Risiken oder weder noch?', 'answers': ['1: RISIKO UEBERWIEGT', '2: EHER RISIKO', '3: WEDER NOCH', '4: EHER CHANCE', '5: CHANCE UEBERWIEGT']}, 'mm01': {'task_type': 'question', 'statement': \"Inwieweit stimmen Sie der folgenden Aussage zu: 'Die Ausübung des islamischen Glaubens in Deutschland sollte eingeschränkt werden.'?\", 'answers': ['1: 1 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '2: 2 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '3: 3 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '4: 4 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '5: 5 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '6: 6 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '7: 7 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")']}, 'vi10': {'task_type': 'question', 'statement': \"Wie wichtig ist es für Sie persönlich 'sich politisch zu engagieren'?\", 'answers': ['1: 1 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '2: 2 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '3: 3 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '4: 4 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '5: 5 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '6: 6 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '7: 7 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")']}, 'vm17': {'task_type': 'question', 'statement': 'Wie beurteilen Sie es, wenn ein Paar anonym gespendete Ei- oder Samenzellen verwendet, um mit medizinischer Hilfe ein Kind zu bekommen?', 'answers': ['1: -3 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '2: -2 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '3: -1 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '4: 0 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '5: 1 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '6: 2 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '7: 3 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")']}, 'st01': {'task_type': 'question', 'statement': 'Manche Leute sagen, dass man den meisten Menschen trauen kann. Andere meinen, dass man nicht vorsichtig genug sein kann im Umgang mit anderen Menschen. Was ist Ihre Meinung dazu?', 'answers': ['1: MAN KANN TRAUEN', '2: MUSS VORSICHTIG SEIN', '3: KOMMT DARAUF AN', '4: SONSTIGES']}, 'iw04': {'task_type': 'question', 'statement': \"Inwiefern stimmen Sie der folgenden Auffassung zu: 'Der Staat muss dafür sorgen, dass man auch bei Krankheit, Not, Arbeitslosigkeit und im Alter ein gutes Auskommen hat.'? (Antwortmöglichkeiten: voll zustimmen, eher zustimmen, eher nicht zustimmen, überhaupt nicht zustimmen)\", 'answers': ['1: STIMME VOLL ZU', '2: STIMME EHER ZU', '3: STIMME EHER NICHT ZU', '4: STIMME GAR NICHT ZU']}, 'ep06': {'task_type': 'question', 'statement': 'Was glauben Sie, wie wird Ihre eigene wirtschaftliche Lage in einem Jahr sein?', 'answers': ['1: WESENTLICH BESSER', '2: ETWAS BESSER', '3: GLEICHBLEIBEND', '4: ETWAS SCHLECHTER', '5: WESENTL. SCHLECHTER']}, 'li03': {'task_type': 'question', 'statement': \"Wie wichtig ist für Sie der Lebensbereich 'Freizeit und Erholung'? Bitte antworten Sie auf einer Skala von 1 (unwichtig) bis 7 (sehr wichtig).\", 'answers': ['1: 1 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '2: 2 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '3: 3 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '4: 4 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '5: 5 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '6: 6 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '7: 7 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")']}, 'ps01': {'task_type': 'question', 'statement': 'Wie zufrieden sind Sie - insgesamt betrachtet - mit den gegenwärtigen Leistungen der Bundesregierung?', 'answers': ['1: SEHR ZUFRIEDEN', '2: ZIEMLICH ZUFRIEDEN', '3: ETWAS ZUFRIEDEN', '4: ETWAS UNZUFRIEDEN', '5: ZIEML. UNZUFRIEDEN', '6: SEHR UNZUFRIEDEN']}, 'mp12': {'task_type': 'question', 'statement': \"Inwieweit stimmen Sie der folgenden Aussage zu: 'Die Ausländer in Deutschland tragen dazu bei, den Fachkräftemangel zu beheben.'?\", 'answers': ['1: 1 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '2: 2 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '3: 3 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '4: 4 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '5: 5 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '6: 6 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '7: 7 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")']}, 'rb01': {'task_type': 'question', 'statement': \"Wie stark stimmen Sie der folgenden Aussage zu: 'Es gibt einen Gott, der sich mit jedem Menschen persönlich befasst.'?\", 'answers': ['1: STIMME VOLL ZU', '2: STIMME EHER ZU', '3: KEINE FESTE MEINUNG', '4: STIMME EHER NICHT ZU', '5: STIMME GAR NICHT ZU', '6: NOCH NIE NACHGEDACHT']}, 'vi06': {'task_type': 'question', 'statement': \"Wie wichtig ist es für Sie persönlich 'sozial Benachteiligten und gesellschaftlichen Randgruppen zu helfen'?\", 'answers': ['1: 1 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '2: 2 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '3: 3 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '4: 4 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '5: 5 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '6: 6 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '7: 7 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")']}, 'ca12': {'task_type': 'question', 'statement': 'Für wie schlimm halten Sie persönlich das folgende Verhalten: Jemand raucht mehrmals in der Woche Haschisch. (Antwortmöglichkeiten: sehr schlimm, ziemlich schlimm, weniger schlimm, überhaupt nicht schlimm)', 'answers': ['1: SEHR SCHLIMM', '2: ZIEMLICH SCHLIMM', '3: WENIGER SCHLIMM', '4: GAR NICHT SCHLIMM']}, 'sm01': {'task_type': 'question', 'statement': 'Sind Sie derzeit Mitglied in einer Gewerkschaft?', 'answers': ['1: JA', '2: NEIN']}, 'im08': {'task_type': 'question', 'statement': \"Wie wichtig ist Ihrer Meinung nach 'Leistung, Fleiß' für den Aufstieg in unserer Gesellschaft gegenwärtig?\", 'answers': ['1: SEHR WICHTIG', '2: WICHTIG', '3: WENIGER WICHTIG', '4: UNWICHTIG']}, 'ep03': {'task_type': 'question', 'statement': 'Wie beurteilen Sie Ihre eigene wirtschaftliche Lage heute?', 'answers': ['1: SEHR GUT', '2: GUT', '3: TEILS/TEILS', '4: SCHLECHT', '5: SEHR SCHLECHT']}, 'lp03': {'task_type': 'question', 'statement': \"Sind Sie bei der folgenden Aussage derselben oder anderer Meinung: 'Egal was manche Leute sagen: Die Situation der einfachen Leute wird nicht besser, sondern schlechter.'?\", 'answers': ['1: BIN DERS.MEINUNG', '2: BIN ANDERER MEINUNG']}, 'pt15': {'task_type': 'question', 'statement': 'Wie groß ist Ihr Vertrauen in die politischen Parteien? (Skala: 1=überhaupt kein Vertrauen, 7=sehr großes Vertrauen)', 'answers': ['1: 1 (1-7 \"GAR KEIN VERTRAUEN\"-\"GROSSES VERTRAUEN\")', '2: 2 (1-7 \"GAR KEIN VERTRAUEN\"-\"GROSSES VERTRAUEN\")', '3: 3 (1-7 \"GAR KEIN VERTRAUEN\"-\"GROSSES VERTRAUEN\")', '4: 4 (1-7 \"GAR KEIN VERTRAUEN\"-\"GROSSES VERTRAUEN\")', '5: 5 (1-7 \"GAR KEIN VERTRAUEN\"-\"GROSSES VERTRAUEN\")', '6: 6 (1-7 \"GAR KEIN VERTRAUEN\"-\"GROSSES VERTRAUEN\")', '7: 7 (1-7 \"GAR KEIN VERTRAUEN\"-\"GROSSES VERTRAUEN\")']}, 'mn01': {'task_type': 'question', 'statement': 'Wie wichtig sollte bei der Vergabe der deutschen Staatsbürgerschaft sein, ob die Person in Deutschland geboren ist?', 'answers': ['1: 1 (1-7 \"GAR NICHT WICHTIG\"-\"SEHR WICHTIG\")', '2: 2 (1-7 \"GAR NICHT WICHTIG\"-\"SEHR WICHTIG\")', '3: 3 (1-7 \"GAR NICHT WICHTIG\"-\"SEHR WICHTIG\")', '4: 4 (1-7 \"GAR NICHT WICHTIG\"-\"SEHR WICHTIG\")', '5: 5 (1-7 \"GAR NICHT WICHTIG\"-\"SEHR WICHTIG\")', '6: 6 (1-7 \"GAR NICHT WICHTIG\"-\"SEHR WICHTIG\")', '7: 7 (1-7 \"GAR NICHT WICHTIG\"-\"SEHR WICHTIG\")']}, 'mm06': {'task_type': 'question', 'statement': \"Inwieweit stimmen Sie der folgenden Aussage zu: 'Ich habe den Eindruck, dass unter den in Deutschland lebenden Muslimen viele religiöse Fanatiker sind.'?\", 'answers': ['1: 1 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '2: 2 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '3: 3 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '4: 4 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '5: 5 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '6: 6 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")', '7: 7 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ ZU\")']}, 'vi07': {'task_type': 'question', 'statement': \"Wie wichtig ist es für Sie persönlich 'sich und seine Bedürfnisse gegen andere durchzusetzen'?\", 'answers': ['1: 1 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '2: 2 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '3: 3 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '4: 4 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '5: 5 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '6: 6 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")', '7: 7 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")']}, 'vm16': {'task_type': 'question', 'statement': 'Wie beurteilen Sie es, wenn ein Paar eigene Ei- und Samenzellen verwendet, um mit medizinischer Hilfe ein Kind zu bekommen?', 'answers': ['1: -3 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '2: -2 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '3: -1 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '4: 0 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '5: 1 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '6: 2 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")', '7: 3 (\"-3\"-\"+3\" \"SEHR FALSCH\"-\"SEHR RICHTIG\")']}, 'sm02': {'task_type': 'question', 'statement': 'Waren Sie früher einmal Mitglied in einer Gewerkschaft?', 'answers': ['1: JA', '2: NEIN']}, 'im03': {'task_type': 'question', 'statement': \"Wie wichtig ist Ihrer Meinung nach 'Bildung, Ausbildung' für den Aufstieg in unserer Gesellschaft gegenwärtig?\", 'answers': ['1: SEHR WICHTIG', '2: WICHTIG', '3: WENIGER WICHTIG', '4: UNWICHTIG']}, 'ep01': {'task_type': 'question', 'statement': 'Wie beurteilen Sie ganz allgemein die heutige wirtschaftliche Lage in Deutschland?', 'answers': ['1: SEHR GUT', '2: GUT', '3: TEILS/TEILS', '4: SCHLECHT', '5: SEHR SCHLECHT']}}\n"
     ]
    }
   ],
   "source": [
    "# import example ALLBUS questions\n",
    "url = \"https://raw.githubusercontent.com/germanpersonas/German-General-Personas/refs/heads/main/_strat_task_question.json\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON content into a dictionary\n",
    "    json_questionnaire = response.json()\n",
    "    \n",
    "    print(\"Successfully loaded as dictionary:\")\n",
    "    print(json_questionnaire)\n",
    "else:\n",
    "    print(f\"Failed to retrieve file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27285424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_type': 'question',\n",
       " 'statement': 'Ergeben sich Ihrer Meinung nach wegen der Flüchtlinge in Bezug auf das Zusammenleben in der Gesellschaft mehr Chancen, mehr Risiken oder weder noch?',\n",
       " 'answers': ['1: RISIKO UEBERWIEGT',\n",
       "  '2: EHER RISIKO',\n",
       "  '3: WEDER NOCH',\n",
       "  '4: EHER CHANCE',\n",
       "  '5: CHANCE UEBERWIEGT']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_questionnaire[\"mp18\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08ca41",
   "metadata": {},
   "source": [
    "We extract the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36dbc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_list = []\n",
    "\n",
    "for key, value in json_questionnaire.items():\n",
    "    # Create a new empty dict for this row\n",
    "    questionnaire_item = {}\n",
    "    \n",
    "    # Update it with the specific format you wanted\n",
    "    questionnaire_item.update({\n",
    "        \"questionnaire_item_id\": key, \n",
    "        \"question_content\": value[\"statement\"]\n",
    "    })\n",
    "    \n",
    "    # Add to the list\n",
    "    questionnaire_list.append(questionnaire_item)\n",
    "questionnaire = pd.DataFrame(questionnaire_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4a282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  questionnaire_item_id                                   question_content\n",
      "0                  lp04  Sind Sie bei der folgenden Aussage derselben o...\n",
      "1                  pe05  Inwiefern stimmen Sie der folgenden Meinung zu...\n",
      "2                  mp18  Ergeben sich Ihrer Meinung nach wegen der Flüc...\n"
     ]
    }
   ],
   "source": [
    "print(questionnaire.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c5af8",
   "metadata": {},
   "source": [
    "We extract the answers and get only their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d91e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cleaned_answers = []\n",
    "for key, value in json_questionnaire.items():\n",
    "    cleaned_answers = []\n",
    "    \n",
    "    for i, answer in enumerate(value[\"answers\"]):\n",
    "        clean_text = answer.split(\": \")[1] \n",
    "\n",
    "        # We simply check if the text contains a minus -> If it does it is a from to scale\n",
    "        if \"-\" in clean_text:\n",
    "            from_to_scale = True\n",
    "            # We only want to have name index\n",
    "            #clean_text = clean_text.split('(')\n",
    "            cleaned_answers.append(clean_text)\n",
    "        else:\n",
    "            from_to_scale = False\n",
    "            cleaned_answers.append(clean_text)\n",
    "    \n",
    "    all_cleaned_answers.append({\"question\": key, \"answer\": cleaned_answers, \"from_to_scale\": from_to_scale})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa1c094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'mp18',\n",
       " 'answer': ['RISIKO UEBERWIEGT',\n",
       "  'EHER RISIKO',\n",
       "  'WEDER NOCH',\n",
       "  'EHER CHANCE',\n",
       "  'CHANCE UEBERWIEGT'],\n",
       " 'from_to_scale': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cleaned_answers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b96e17",
   "metadata": {},
   "source": [
    "## System Prompt, User Prompt and Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76bba3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Nehme die Perspektive der folgenden Person ein: {persona}\"\n",
    "prompt = (\n",
    "    f\"Welche der Antwortmöglichkeiten ist die Reaktion der Person auf folgende Frage: {placeholder.PROMPT_QUESTIONS}\\n\"\n",
    "    f\"{placeholder.PROMPT_OPTIONS}\\n\"\n",
    "    f\"{placeholder.PROMPT_AUTOMATIC_OUTPUT_INSTRUCTIONS}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf5621",
   "metadata": {},
   "source": [
    "## Different Ways to get the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a2776a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_METHODS = [\"OPEN\", \"RESTRICTED_CHOICE\", \"REASONING_JSON\", \"VERBALIZED_DISTRIBUTION\"]\n",
    "# Select your method here by copying one from above\n",
    "output_method = \"RESTRICTED_CHOICE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854455f9",
   "metadata": {},
   "source": [
    "### Creating the LLM Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "329a624d-02aa-4e00-b366-eb5f0354c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_prompts(row: pd.Series):\n",
    "    persona_index = row.name\n",
    "    persona_str = row[\"persona\"]\n",
    "\n",
    "    # We create a LLMPrompt for each persona\n",
    "    llm_prompt = LLMPrompt(\n",
    "        questionnaire_source=questionnaire,\n",
    "        questionnaire_name=str(persona_index),\n",
    "        system_prompt=system_prompt.format(persona=persona_str),\n",
    "        prompt=prompt\n",
    "    )\n",
    "\n",
    "    # Here we define how the LLM should answer the question\n",
    "    answer_options = {}\n",
    "    \n",
    "    for dic in all_cleaned_answers:\n",
    "        answers = dic[\"answer\"]\n",
    "        from_to_scale = dic[\"from_to_scale\"]\n",
    "        rgm: response_generation.ResponseGenerationMethod = None\n",
    "\n",
    "        \n",
    "        # We change the ResponseGenerationMethod here. All other code stays the same\n",
    "        if output_method == \"OPEN\":\n",
    "            pass            \n",
    "        elif output_method == \"RESTRICTED_CHOICE\":\n",
    "            rgm = response_generation.ChoiceResponseGenerationMethod(answers, output_template=f\"Antworte nur mit der exakten Antwort.\")\n",
    "        elif output_method == \"REASONING_JSON\":\n",
    "            rgm = response_generation.JSONReasoningResponseGenerationMethod(output_template=f\"Antworte nur im folgenden JSON format:\\n{placeholder.JSON_TEMPLATE}\")\n",
    "        elif output_method == \"VERBALIZED_DISTRIBUTION\":\n",
    "            rgm = response_generation.JSONVerbalizedDistribution(output_template=f\"Gib für jede Antwortmöglichkeit eine Wahrscheinlichkeit an, mit der die Person antwortet. Nutze dafür folgendes JSON format:\\n{placeholder.JSON_TEMPLATE}\")\n",
    "\n",
    "        # We can check for robustness with generate_likert_options: \n",
    "        # Randomized or reversed options order, different indeces etc.\n",
    "        if from_to_scale:\n",
    "            answer_option = generate_likert_options(n=len(answers), answer_texts=answers, only_from_to_scale=True, scale_prompt_template=\"Antwortmöglichkeiten: {start} bis {end}\", response_generation_method=rgm)\n",
    "        else:\n",
    "            answer_option = generate_likert_options(n=len(answers), answer_texts=answers, list_prompt_template=\"Antwortmöglichkeiten: {options}\" , response_generation_method=rgm)\n",
    "        answer_options[dic[\"question\"]]= answer_option\n",
    "\n",
    "    llm_prompt.prepare_prompt(answer_options=answer_options)\n",
    "    return llm_prompt\n",
    "\n",
    "llm_prompts: list[LLMPrompt] = df_personas.apply(create_llm_prompts, axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ff3a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: Nehme die Perspektive der folgenden Person ein: Du bist eine Person, 26 Jahre alt, männlich und wohnst in Westdeutschland in einem Dorf. Du hast die Fachhochschulreife und einen Universitätsabschluss (Diplom). Dein monatliches Nettoeinkommen liegt zwischen 2000 und 2249 Euro. Du bist angestellt und besitzt die spanische und ungarische Staatsangehörigkeit.\n",
      "USER: Welche der Antwortmöglichkeiten ist die Reaktion der Person auf folgende Frage: Ergeben sich Ihrer Meinung nach wegen der Flüchtlinge in Bezug auf das Zusammenleben in der Gesellschaft mehr Chancen, mehr Risiken oder weder noch?\n",
      "Antwortmöglichkeiten: 1: RISIKO UEBERWIEGT, 2: EHER RISIKO, 3: WEDER NOCH, 4: EHER CHANCE, 5: CHANCE UEBERWIEGT\n",
      "Antworte nur mit der exakten Antwort.\n"
     ]
    }
   ],
   "source": [
    "sys_prompt, user_prompt = llm_prompts[0].get_prompt_for_questionnaire_type(item_id=\"mp18\")\n",
    "print(\"SYSTEM:\", sys_prompt)\n",
    "print(\"USER:\", user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2fa25",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "252cc3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "generator = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "611a8d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questionnaires:   0%|          | 0/27 [00:00<?, ?it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A/mnt/homes/jens/.conda/envs/jens_qstn/lib/python3.12/site-packages/qstn/inference/remote_inference.py:229: UserWarning: Strict Choice Response Generation is only supported for vllm APIs.\n",
      "  warnings.warn(\"Strict Choice Response Generation is only supported for vllm APIs.\")\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:01<00:22,  1.21s/it]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:01<00:00, 10.62it/s]\u001b[A\n",
      "Processing questionnaires:   4%|▎         | 1/27 [00:01<00:49,  1.89s/it]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:37] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:10,  1.84it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 15.37it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Processing questionnaires:   7%|▋         | 2/27 [00:02<00:31,  1.25s/it]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  2.06it/s]\u001b[A\n",
      "Processing Prompts:  15%|█▌        | 3/20 [00:00<00:02,  6.14it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 26.02it/s]\u001b[A\n",
      "Processing questionnaires:  11%|█         | 3/27 [00:03<00:24,  1.03s/it]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:38] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:13,  1.36it/s]\u001b[A\n",
      "Processing Prompts:  45%|████▌     | 9/20 [00:00<00:00, 13.86it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:01<00:00, 17.47it/s]\u001b[A\n",
      "Processing questionnaires:  15%|█▍        | 4/27 [00:04<00:24,  1.08s/it]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:39] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:12,  1.54it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 13.56it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 20.08it/s]\u001b[A\n",
      "Processing questionnaires:  19%|█▊        | 5/27 [00:05<00:23,  1.05s/it]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:40] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:13,  1.45it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 13.08it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:01<00:00, 18.52it/s]\u001b[A\n",
      "Processing questionnaires:  22%|██▏       | 6/27 [00:06<00:22,  1.07s/it]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  1.98it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 15.82it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 26.84it/s]\u001b[A\n",
      "Processing questionnaires:  26%|██▌       | 7/27 [00:07<00:19,  1.04it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:42] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  1.91it/s]\u001b[A\n",
      "Processing Prompts:  30%|███       | 6/20 [00:00<00:01, 12.17it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 26.12it/s]\u001b[A\n",
      "Processing questionnaires:  30%|██▉       | 8/27 [00:08<00:17,  1.11it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:43] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  1.93it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 27.11it/s]\u001b[A\n",
      "Processing questionnaires:  33%|███▎      | 9/27 [00:08<00:15,  1.17it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:44] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:12,  1.57it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 13.43it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 20.26it/s]\u001b[A\n",
      "Processing questionnaires:  37%|███▋      | 10/27 [00:09<00:15,  1.12it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:45] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:10,  1.89it/s]\u001b[A\n",
      "Processing Prompts:  30%|███       | 6/20 [00:00<00:01, 11.86it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Processing questionnaires:  41%|████      | 11/27 [00:10<00:13,  1.16it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:46] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:13,  1.42it/s]\u001b[A\n",
      "Processing Prompts:  45%|████▌     | 9/20 [00:00<00:00, 14.38it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:01<00:00, 18.02it/s]\u001b[A\n",
      "Processing questionnaires:  44%|████▍     | 12/27 [00:11<00:14,  1.06it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  1.91it/s]\u001b[A\n",
      "Processing Prompts:  35%|███▌      | 7/20 [00:00<00:00, 14.25it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 25.86it/s]\u001b[A\n",
      "Processing questionnaires:  48%|████▊     | 13/27 [00:12<00:12,  1.12it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:47] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:11,  1.61it/s]\u001b[A\n",
      "Processing Prompts:  35%|███▌      | 7/20 [00:00<00:01, 12.40it/s]\u001b[A\n",
      "Processing Prompts:  55%|█████▌    | 11/20 [00:00<00:00, 16.68it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 20.56it/s]\u001b[A\n",
      "Processing questionnaires:  52%|█████▏    | 14/27 [00:13<00:11,  1.09it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:48] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  1.97it/s]\u001b[A\n",
      "Processing Prompts:  30%|███       | 6/20 [00:00<00:01, 12.42it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 27.50it/s]\u001b[A\n",
      "Processing questionnaires:  56%|█████▌    | 15/27 [00:14<00:10,  1.16it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:49] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:08,  2.17it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 33.35it/s]\u001b[A\n",
      "Processing questionnaires:  59%|█████▉    | 16/27 [00:14<00:08,  1.27it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:50] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  1.96it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 28.08it/s]\u001b[A\n",
      "Processing questionnaires:  63%|██████▎   | 17/27 [00:15<00:07,  1.31it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  1.99it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 16.80it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 27.71it/s]\u001b[A\n",
      "Processing questionnaires:  67%|██████▋   | 18/27 [00:16<00:06,  1.33it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:51] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  1.91it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 16.36it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 27.29it/s]\u001b[A\n",
      "Processing questionnaires:  70%|███████   | 19/27 [00:17<00:05,  1.33it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:52] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:13,  1.43it/s]\u001b[A\n",
      "Processing Prompts:  50%|█████     | 10/20 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:01<00:00, 19.02it/s]\u001b[A\n",
      "Processing questionnaires:  74%|███████▍  | 20/27 [00:18<00:05,  1.19it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:53] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:12,  1.47it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 12.85it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:01<00:00, 18.83it/s]\u001b[A\n",
      "Processing questionnaires:  78%|███████▊  | 21/27 [00:19<00:05,  1.10it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:54] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:13,  1.40it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 12.67it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:01<00:00, 17.84it/s]\u001b[A\n",
      "Processing questionnaires:  81%|████████▏ | 22/27 [00:20<00:04,  1.03it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:55] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:12,  1.52it/s]\u001b[A\n",
      "Processing Prompts:  35%|███▌      | 7/20 [00:00<00:01, 11.98it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 20.08it/s]\u001b[A\n",
      "Processing questionnaires:  85%|████████▌ | 23/27 [00:21<00:03,  1.02it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:56] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:12,  1.49it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 12.89it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:01<00:00, 18.99it/s]\u001b[A\n",
      "Processing questionnaires:  89%|████████▉ | 24/27 [00:22<00:03,  1.01s/it]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:57] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:08,  2.23it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 34.14it/s]\u001b[A\n",
      "Processing questionnaires:  93%|█████████▎| 25/27 [00:23<00:01,  1.13it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:58] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:08,  2.11it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 30.33it/s][A\n",
      "Processing questionnaires:  96%|█████████▋| 26/27 [00:23<00:00,  1.23it/s]\n",
      "Processing Prompts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.420072 seconds\n",
      "[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496874 seconds\n",
      "[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.465621 seconds\n",
      "[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.472099 seconds\n",
      "[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.407941 seconds\n",
      "[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.415413 seconds\n",
      "[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.388478 seconds\n",
      "[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.489133 seconds\n",
      "[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.447260 seconds\n",
      "[2026-01-23 12:22:59] INFO _base_client.py:1621: Retrying request to /chat/completions in 0.496275 seconds\n",
      "\n",
      "Processing Prompts:   5%|▌         | 1/20 [00:00<00:09,  1.93it/s]\u001b[A\n",
      "Processing Prompts:  40%|████      | 8/20 [00:00<00:00, 15.92it/s]\u001b[A\n",
      "Processing Prompts: 100%|██████████| 20/20 [00:00<00:00, 26.82it/s]\u001b[A\n",
      "Processing questionnaires: 100%|██████████| 27/27 [00:24<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "results = conduct_survey_single_item(\n",
    "    generator, llm_prompts=llm_prompts, client_model_name=model_id, max_tokens=2000, seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ddd4ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionnaire_name</th>\n",
       "      <th>questionnaire_item_id</th>\n",
       "      <th>question</th>\n",
       "      <th>llm_response</th>\n",
       "      <th>logprobs</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lp04</td>\n",
       "      <td>Sind Sie bei der folgenden Aussage derselben o...</td>\n",
       "      <td>BIN ANDERER MEINUNG</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>pe05</td>\n",
       "      <td>Inwiefern stimmen Sie der folgenden Meinung zu...</td>\n",
       "      <td>STIMME EHER NICHT ZU</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mp18</td>\n",
       "      <td>Ergeben sich Ihrer Meinung nach wegen der Flüc...</td>\n",
       "      <td>EHER RISIKO</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>mm01</td>\n",
       "      <td>Inwieweit stimmen Sie der folgenden Aussage zu...</td>\n",
       "      <td>2 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>vi10</td>\n",
       "      <td>Wie wichtig ist es für Sie persönlich 'sich po...</td>\n",
       "      <td>4 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  questionnaire_name questionnaire_item_id  \\\n",
       "0                  0                  lp04   \n",
       "1                  0                  pe05   \n",
       "2                  0                  mp18   \n",
       "3                  0                  mm01   \n",
       "4                  0                  vi10   \n",
       "\n",
       "                                            question  \\\n",
       "0  Sind Sie bei der folgenden Aussage derselben o...   \n",
       "1  Inwiefern stimmen Sie der folgenden Meinung zu...   \n",
       "2  Ergeben sich Ihrer Meinung nach wegen der Flüc...   \n",
       "3  Inwieweit stimmen Sie der folgenden Aussage zu...   \n",
       "4  Wie wichtig ist es für Sie persönlich 'sich po...   \n",
       "\n",
       "                                        llm_response logprobs reasoning  \n",
       "0                                BIN ANDERER MEINUNG     None      None  \n",
       "1                               STIMME EHER NICHT ZU     None      None  \n",
       "2                                        EHER RISIKO     None      None  \n",
       "3  2 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ...     None      None  \n",
       "4                 4 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")     None      None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we expect JSON output we can automatically parse it\n",
    "if output_method == \"REASONING_JSON\" or output_method == \"VERBALIZED_DISTRIBUTION\":\n",
    "    parsed_results = parse_json(results)\n",
    "else:\n",
    "    parsed_results = raw_responses(results)\n",
    "\n",
    "full_results = create_one_dataframe(parsed_results)\n",
    "full_results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jens_qstn",
   "language": "python",
   "name": "jens_qstn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
