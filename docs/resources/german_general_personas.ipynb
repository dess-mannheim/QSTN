{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e828eeca",
   "metadata": {},
   "source": [
    "# German General Personas\n",
    "\n",
    "**This resource allows you to ask questions to a representative sample of 5,246 individual personas representing the German population.**\n",
    "\n",
    "This persona collection consists of **5,246 individual personas representing the German population**. The data source is the [German General Survey (ALLBUScompact)](https://www.gesis.org/en/allbus). Their two-step randomized sampling ensures that ALLBUS as well as the **German General Personas** reflect a representative picture of the German population, regarding its sociodemographic attributes, norms and values.\n",
    "\n",
    "The persona collection was first published in the work [German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies](https://www.arxiv.org/abs/2511.21722).\n",
    "\n",
    "\n",
    "## Why German General Personas? \n",
    "\n",
    "GGP offers several significant advantages:\n",
    "\n",
    "- **Contextual Information**: Personas enrich language models with relevant contextual information, enabling them to anchor predictions for specific tasks or target variables in empirically observed associations and connections within the German population.\n",
    "- **Representative Alignment**: The ALLBUS is a probability-based survey, and the personas derived from it are designed to represent the German population accurately. While there's growing concern about biased representations in LLMs' survey responses, GGP can potentially help align LLMs more effectively with the demographics and attitudes of the German population.\n",
    "- **Novel Resource**: GGP stands as a novel textual resource for researchers and practitioners in Natural Language Processing (NLP) and Computational Social Science (CSS).\n",
    "\n",
    "We show how GPP can easily be utilized with the `qstn` Framework. The Tutorial here is also available as an interactive [jupyter notebook](https://github.com/dess-mannheim/QSTN/blob/main/docs/resources/german_general_personas.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac98f97",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "First, all relevant Python packages must be imported, such as pandas and **QSTN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc84810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Either local inference with vllm or remote with AsyncOpenAI\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qstn Imports\n",
    "from qstn.survey_manager import conduct_survey_single_item, conduct_survey_sequential\n",
    "from qstn.parser import parse_json, raw_responses\n",
    "from qstn.utilities import create_one_dataframe\n",
    "\n",
    "from qstn.prompt_builder import LLMPrompt, generate_likert_options\n",
    "from qstn.utilities import placeholder\n",
    "\n",
    "from qstn.inference import response_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38545590",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d1097",
   "metadata": {},
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa5b8c",
   "metadata": {},
   "source": [
    "We will load the personas from the [GGP repository](https://github.com/germanpersonas/German-General-Personas).\n",
    "\n",
    "You can define how many personas you want to load and conduct interviews with. In total, you can choose up to 5,246 personas. If you choose less than the available amount of personas, we randomly select personas to ensure a representative sample of the GGP collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a42398",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONAS_TO_LOAD = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c73f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 rows.\n",
      "   index                                            persona\n",
      "0   3811  Du bist eine Person, 26 Jahre alt, männlich un...\n",
      "1   2976  Du bist eine Person, 42 Jahre alt, weiblich, m...\n",
      "2   1387  Du bist eine Person, 38 Jahre alt, weiblich un...\n",
      "3   3773  Du bist eine Person, 54 Jahre alt, wohnhaft in...\n",
      "4   2803  Du bist eine Person, 20 Jahre alt, weiblich, a...\n"
     ]
    }
   ],
   "source": [
    "zip_url = \"https://github.com/germanpersonas/German-General-Personas/raw/main/GGP_all_topk_fulltext.zip\"\n",
    "\n",
    "response = requests.get(zip_url)\n",
    "response.raise_for_status()  # Check for errors\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    with z.open(\"pc_fulltext_sociodemographics_only.jsonl\") as f:\n",
    "        if PERSONAS_TO_LOAD:\n",
    "            df_personas = pd.read_json(f, lines=True)\n",
    "            df_personas = df_personas.sample(n=PERSONAS_TO_LOAD).reset_index(drop=False)\n",
    "        else:\n",
    "            df_personas = pd.read_json(f, lines=True)\n",
    "\n",
    "df_personas = df_personas.rename(columns={0: \"persona\"})\n",
    "print(f\"Loaded {len(df_personas)} rows.\")\n",
    "print(df_personas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c85a1-3962-4950-b354-ae6c4509597d",
   "metadata": {},
   "source": [
    "In addition to the personas, we load example questions and answer options to conduct the survey. \n",
    "\n",
    "However, with **QSTN you can freely create or choose the questions you are asking a representative sample of the German population**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c7f4a",
   "metadata": {},
   "source": [
    "## Create your own questions\n",
    "\n",
    "If you want to create your owm questions, you can simply create a json in the following format, which contains the question in `statement`, and the possible answers in ``answers``. If you then want to ask your own questions to the LLM, set `USE_OWN_QUESTIONS` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ec1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this if you want to use your own questions\n",
    "USE_OWN_QUESTIONS: bool = False\n",
    "\n",
    "if USE_OWN_QUESTIONS:\n",
    "    json_questionnaire = {\n",
    "        \"controversial\": {\n",
    "            \"task_type\": \"question\",\n",
    "            \"statement\": \"Wie stehen Sie zu Ananas auf Pizza?\",\n",
    "            \"answers\": [\"1: Lecker!\", \"2: Schrecklich!\"],\n",
    "        },\n",
    "        \"election\": {\n",
    "            \"task_type\": \"question\",\n",
    "            \"statement\": \"Welche Partei würden Sie heute wählen?\",\n",
    "            \"answers\": [\n",
    "                \"1: CDU/CSU\",\n",
    "                \"2: SPD\",\n",
    "                \"3: FDP\",\n",
    "                \"4: Bündnis90/Die Grünen\",\n",
    "                \"5: Die Linke\",\n",
    "                \"6: AFD\",\n",
    "                \"7: Sonstige\"\n",
    "                \"8: Ich wähle nicht.\"\n",
    "            ],\n",
    "        },\n",
    "        # You can add as many questions as you want\n",
    "    }\n",
    "    print(\"Using your own questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1c7d7",
   "metadata": {},
   "source": [
    "## ALLBUS Example Questions\n",
    "\n",
    "If you don't want to use your own questions, you can use the questions of the ALLBUS, which were used in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169d86a-4c1e-4eaf-ac6c-93a698860146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded as dictionary. Using ALLBUS questions.\n"
     ]
    }
   ],
   "source": [
    "if not USE_OWN_QUESTIONS:\n",
    "    # import example ALLBUS questions\n",
    "    url = \"https://raw.githubusercontent.com/germanpersonas/German-General-Personas/refs/heads/main/_strat_task_question.json\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_questionnaire = response.json()\n",
    "\n",
    "        print(\"Successfully loaded as dictionary. Using ALLBUS questions.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve file. Status code: {response.status_code}\")\n",
    "    print(json_questionnaire[\"mp18\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08ca41",
   "metadata": {},
   "source": [
    "To use the questions in the correct format, we have to adjust them to the qstn format and add them to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dbc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_list = []\n",
    "\n",
    "for key, value in json_questionnaire.items():\n",
    "    # Create a new empty dict for this row\n",
    "    questionnaire_item = {}\n",
    "\n",
    "    # Update it with the specific format you wanted\n",
    "    questionnaire_item.update(\n",
    "        {\"questionnaire_item_id\": key, \"question_content\": value[\"statement\"]}\n",
    "    )\n",
    "\n",
    "    # Add to the list\n",
    "    questionnaire_list.append(questionnaire_item)\n",
    "questionnaire = pd.DataFrame(questionnaire_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4a282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  questionnaire_item_id                                   question_content\n",
      "0                  lp04  Sind Sie bei der folgenden Aussage derselben o...\n",
      "1                  pe05  Inwiefern stimmen Sie der folgenden Meinung zu...\n",
      "2                  mp18  Ergeben sich Ihrer Meinung nach wegen der Flüc...\n"
     ]
    }
   ],
   "source": [
    "print(questionnaire.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c5af8",
   "metadata": {},
   "source": [
    "For the answers, we can select if they are on a scale or if they are categorical. We only need the plaintext answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cleaned_answers = []\n",
    "for key, value in json_questionnaire.items():\n",
    "    cleaned_answers = []\n",
    "\n",
    "    for i, answer in enumerate(value[\"answers\"]):\n",
    "        clean_text = answer.split(\": \")[1]\n",
    "\n",
    "        # We simply check if the text contains a minus -> If it does it is a from to scale\n",
    "        if \"-\" in clean_text:\n",
    "            from_to_scale = True\n",
    "            cleaned_answers.append(clean_text)\n",
    "        else:\n",
    "            from_to_scale = False\n",
    "            cleaned_answers.append(clean_text)\n",
    "\n",
    "    all_cleaned_answers.append(\n",
    "        {\"question\": key, \"answer\": cleaned_answers, \"from_to_scale\": from_to_scale}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa1c094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'mp18',\n",
       " 'answer': ['RISIKO UEBERWIEGT',\n",
       "  'EHER RISIKO',\n",
       "  'WEDER NOCH',\n",
       "  'EHER CHANCE',\n",
       "  'CHANCE UEBERWIEGT'],\n",
       " 'from_to_scale': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cleaned_answers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b96e17",
   "metadata": {},
   "source": [
    "## System Prompt, User Prompt and Personas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648466f2",
   "metadata": {},
   "source": [
    "Here we define the prompt structure for the interaction.\n",
    "\n",
    "*   **`system_prompt`**: Instructions for the model to adopt the specific `{persona}`.\n",
    "*   **`prompt`**: The main task input which dynamically assembles:\n",
    "    *   The question for each entry in our questionnaire (`PROMPT_QUESTIONS`)\n",
    "    *   The answer choices (`PROMPT_OPTIONS`)\n",
    "    *   The specific formatting rules (`PROMPT_AUTOMATIC_OUTPUT_INSTRUCTIONS`) based on the `output_method` selected in the next step. \n",
    "    \n",
    "We use the prompt from the GGP Paper in this case. Again you can adjust this however you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76bba3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Nehme die Perspektive der folgenden Person ein: {persona}\"\n",
    "prompt = (\n",
    "    f\"Welche der Antwortmöglichkeiten ist die Reaktion der Person auf folgende Frage: {placeholder.PROMPT_QUESTIONS}\\n\"\n",
    "    f\"{placeholder.PROMPT_OPTIONS}\\n\"\n",
    "    f\"{placeholder.PROMPT_AUTOMATIC_OUTPUT_INSTRUCTIONS}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf31a84",
   "metadata": {},
   "source": [
    "## Configuration: Output Method\n",
    "\n",
    "Select the inference technique by assigning one of the keys below to `output_method`.\n",
    "\n",
    "| Method | Description |\n",
    "| :--- | :--- |\n",
    "| **`OPEN`** | Full, unconstrained text generation. |\n",
    "| **`RESTRICTED_CHOICE`** | Logits are restricted to exact answer possibilities only. |\n",
    "| **`REASONING_JSON`** | JSON output containing a preliminary reasoning step. |\n",
    "| **`VERBALIZED_DISTRIBUTION`** | Uncertainty estimation via verbalized probability as in [Meister et al.](https://arxiv.org/abs/2510.01171). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2776a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_METHODS = [\n",
    "    \"OPEN\",\n",
    "    \"RESTRICTED_CHOICE\",\n",
    "    \"REASONING_JSON\",\n",
    "    \"VERBALIZED_DISTRIBUTION\",\n",
    "]\n",
    "# Select your method here by copying one from above\n",
    "output_method = \"RESTRICTED_CHOICE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854455f9",
   "metadata": {},
   "source": [
    "### Creating the LLM Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a624d-02aa-4e00-b366-eb5f0354c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_prompts(row: pd.Series):\n",
    "    persona_index = row.name\n",
    "    persona_str = row[\"persona\"]\n",
    "\n",
    "    # We create a LLMPrompt for each persona\n",
    "    llm_prompt = LLMPrompt(\n",
    "        questionnaire_source=questionnaire,\n",
    "        questionnaire_name=str(persona_index),\n",
    "        system_prompt=system_prompt.format(persona=persona_str),\n",
    "        prompt=prompt,\n",
    "    )\n",
    "\n",
    "    # Here we define how the LLM should answer the question\n",
    "    answer_options = {}\n",
    "\n",
    "    for dic in all_cleaned_answers:\n",
    "        answers = dic[\"answer\"]\n",
    "        from_to_scale = dic[\"from_to_scale\"]\n",
    "        rgm: response_generation.ResponseGenerationMethod = None\n",
    "\n",
    "        # We change the ResponseGenerationMethod here. All other code stays the same\n",
    "        if output_method == \"OPEN\":\n",
    "            pass\n",
    "        elif output_method == \"RESTRICTED_CHOICE\":\n",
    "            rgm = response_generation.ChoiceResponseGenerationMethod(\n",
    "                answers, output_template=f\"Antworte nur mit der exakten Antwort.\"\n",
    "            )\n",
    "        elif output_method == \"REASONING_JSON\":\n",
    "            rgm = response_generation.JSONReasoningResponseGenerationMethod(\n",
    "                output_template=f\"Antworte nur im folgenden JSON format:\\n{placeholder.JSON_TEMPLATE}\"\n",
    "            )\n",
    "        elif output_method == \"VERBALIZED_DISTRIBUTION\":\n",
    "            rgm = response_generation.JSONVerbalizedDistribution(\n",
    "                output_template=f\"Gib für jede Antwortmöglichkeit eine Wahrscheinlichkeit an, mit der die Person antwortet. Nutze dafür folgendes JSON format:\\n{placeholder.JSON_TEMPLATE}\"\n",
    "            )\n",
    "\n",
    "        # We can check for robustness with generate_likert_options:\n",
    "        # Randomized or reversed options order, different indeces etc.\n",
    "        if from_to_scale:\n",
    "            answer_option = generate_likert_options(\n",
    "                n=len(answers),\n",
    "                answer_texts=answers,\n",
    "                only_from_to_scale=True,\n",
    "                scale_prompt_template=\"Antwortmöglichkeiten: {start} bis {end}\",\n",
    "                response_generation_method=rgm,\n",
    "            )\n",
    "        else:\n",
    "            answer_option = generate_likert_options(\n",
    "                n=len(answers),\n",
    "                answer_texts=answers,\n",
    "                list_prompt_template=\"Antwortmöglichkeiten: {options}\",\n",
    "                response_generation_method=rgm,\n",
    "            )\n",
    "        answer_options[dic[\"question\"]] = answer_option\n",
    "\n",
    "    llm_prompt.prepare_prompt(answer_options=answer_options)\n",
    "    return llm_prompt\n",
    "\n",
    "\n",
    "llm_prompts: list[LLMPrompt] = df_personas.apply(create_llm_prompts, axis=1).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d0df6e",
   "metadata": {},
   "source": [
    "Here we can see our final system and user prompts depending on the method we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff3a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: Nehme die Perspektive der folgenden Person ein: Du bist eine Person, 26 Jahre alt, männlich und wohnst in Westdeutschland in einem Dorf. Du hast die Fachhochschulreife und einen Universitätsabschluss (Diplom). Dein monatliches Nettoeinkommen liegt zwischen 2000 und 2249 Euro. Du bist angestellt und besitzt die spanische und ungarische Staatsangehörigkeit.\n",
      "USER: Welche der Antwortmöglichkeiten ist die Reaktion der Person auf folgende Frage: Ergeben sich Ihrer Meinung nach wegen der Flüchtlinge in Bezug auf das Zusammenleben in der Gesellschaft mehr Chancen, mehr Risiken oder weder noch?\n",
      "Antwortmöglichkeiten: 1: RISIKO UEBERWIEGT, 2: EHER RISIKO, 3: WEDER NOCH, 4: EHER CHANCE, 5: CHANCE UEBERWIEGT\n",
      "Antworte nur mit der exakten Antwort.\n"
     ]
    }
   ],
   "source": [
    "sys_prompt, user_prompt = llm_prompts[0].get_prompt_for_questionnaire_type(\n",
    "    item_id=\"mp18\"\n",
    ")\n",
    "print(\"SYSTEM:\", sys_prompt)\n",
    "print(\"USER:\", user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2fa25",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cea7f7",
   "metadata": {},
   "source": [
    "First we initialize our way to inference the model. We use `AsyncOpenAI` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "252cc3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "generator = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f60806",
   "metadata": {},
   "source": [
    "And `QSTN` inferences each question. We use `max_tokens=2000` here to restrict overly long output.\n",
    "If you want previous questions to influence the answers of further questions you can use the method `conduct_survey_sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a8d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = conduct_survey_single_item(\n",
    "    generator,\n",
    "    llm_prompts=llm_prompts,\n",
    "    client_model_name=model_id,\n",
    "    max_tokens=2000,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d6a2f",
   "metadata": {},
   "source": [
    "Finally we can parse the output and get a pandas DataFrame with the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ddd4ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionnaire_name</th>\n",
       "      <th>questionnaire_item_id</th>\n",
       "      <th>question</th>\n",
       "      <th>llm_response</th>\n",
       "      <th>logprobs</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lp04</td>\n",
       "      <td>Sind Sie bei der folgenden Aussage derselben o...</td>\n",
       "      <td>BIN ANDERER MEINUNG</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>pe05</td>\n",
       "      <td>Inwiefern stimmen Sie der folgenden Meinung zu...</td>\n",
       "      <td>STIMME EHER NICHT ZU</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mp18</td>\n",
       "      <td>Ergeben sich Ihrer Meinung nach wegen der Flüc...</td>\n",
       "      <td>EHER RISIKO</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>mm01</td>\n",
       "      <td>Inwieweit stimmen Sie der folgenden Aussage zu...</td>\n",
       "      <td>2 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>vi10</td>\n",
       "      <td>Wie wichtig ist es für Sie persönlich 'sich po...</td>\n",
       "      <td>4 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  questionnaire_name questionnaire_item_id  \\\n",
       "0                  0                  lp04   \n",
       "1                  0                  pe05   \n",
       "2                  0                  mp18   \n",
       "3                  0                  mm01   \n",
       "4                  0                  vi10   \n",
       "\n",
       "                                            question  \\\n",
       "0  Sind Sie bei der folgenden Aussage derselben o...   \n",
       "1  Inwiefern stimmen Sie der folgenden Meinung zu...   \n",
       "2  Ergeben sich Ihrer Meinung nach wegen der Flüc...   \n",
       "3  Inwieweit stimmen Sie der folgenden Aussage zu...   \n",
       "4  Wie wichtig ist es für Sie persönlich 'sich po...   \n",
       "\n",
       "                                        llm_response logprobs reasoning  \n",
       "0                                BIN ANDERER MEINUNG     None      None  \n",
       "1                               STIMME EHER NICHT ZU     None      None  \n",
       "2                                        EHER RISIKO     None      None  \n",
       "3  2 (1-7 \"STIMME GAR NICHT ZU\"-\"STIMME VOLL+GANZ...     None      None  \n",
       "4                 4 (1-7 \"UNWICHTIG\"-\"SEHR WICHTIG\")     None      None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we expect JSON output we can automatically parse it\n",
    "if output_method == \"REASONING_JSON\" or output_method == \"VERBALIZED_DISTRIBUTION\":\n",
    "    parsed_results = parse_json(results)\n",
    "else:\n",
    "    parsed_results = raw_responses(results)\n",
    "\n",
    "full_results = create_one_dataframe(parsed_results)\n",
    "full_results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qstn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
